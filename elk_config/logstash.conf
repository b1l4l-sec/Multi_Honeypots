input {
  file {
    path => "/var/log/multi_honeypot/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "plain"
  }
}

filter {
  # Use 'else if' for better performance.
  # Once a log matches 'ssh', it won't be checked against 'ftp' or 'http'.
  
  # 1. SSH log parsing
  if [message] =~ "ssh" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{IP:ip_address} %{WORD:event_type} %{GREEDYDATA:event_data}" }
      # Remove the original message to fix duplicates
      remove_field => ["message"]
    }
    mutate { add_tag => ["ssh"] }
  }
  
  # 2. FTP log parsing (JSON)
  else if [message] =~ "ftp" {
    json { 
      source => "message" 
      # Remove the original message to fix duplicates
      remove_field => ["message"]
    }

    # This ruby code moves nested fields to the top level
    # This fixes the mapping conflict between 'event_data' (string) and 'event_data' (object)
    ruby {
      code => '
        event_data = event.get("event_data")
        if event_data.is_a?(Hash)
          event_data.each do |key, value|
            event.set(key, value)
          end
          event.remove("event_data")
        end
      '
    }
    mutate { add_tag => ["ftp"] }
  }

  # 3. HTTP log parsing (JSON) - NEW BLOCK
  else if [message] =~ "http" {
    json { 
      source => "message" 
      # Remove the original message to fix duplicates
      remove_field => ["message"]
    }

    # We apply the same ruby logic here in case http logs also have a nested 'event_data'
    ruby {
      code => '
        event_data = event.get("event_data")
        if event_data.is_a?(Hash)
          event_data.each do |key, value|
            event.set(key, value)
          end
          event.remove("event_data")
        end
      '
    }
    mutate { add_tag => ["http"] }
  }
  
  # --- Common Filters (Applied after parsing) ---

  # Date parsing
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
    }
  }
  
  # GeoIP enrichment
  if [ip_address] {
    geoip {
      source => "ip_address"
      target => "geoip"
    }
    
    # Restructure geoip fields to match the template
    ruby {
      code => '
        geoip = event.get("geoip")
        if geoip && geoip["location"]
          event.set("[geoip][geo][location]", {
            "lat" => geoip["location"]["lat"],
            "lon" => geoip["location"]["lon"]
          })
          event.set("[geoip][geo][country_name]", geoip["country_name"]) if geoip["country_name"]
          event.set("[geoip][geo][city_name]", geoip["city_name"]) if geoip["city_name"]
          event.set("[geoip][geo][region_name]", geoip["region_name"]) if geoip["region_name"]
          event.set("[geoip][geo][country_iso_code]", geoip["country_code2"]) if geoip["country_code2"]
          event.set("[geoip][geo][continent_code]", geoip["continent_code"]) if geoip["continent_code"]
          event.set("[geoip][geo][timezone]", geoip["timezone"]) if geoip["timezone"]
          event.set("[geoip][ip]", event.get("ip_address"))
        end
      '
    }
  }

  # Final cleanup to remove the other duplicate field
  mutate {
    remove_field => ["event.original"]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "honeypot-%{+YYYY.MM.dd}"
  }
  stdout { codec => rubydebug }
}
